// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
// versions:
// - protoc-gen-go-grpc v1.2.0
// - protoc             v3.20.3
// source: github.com/metaprov/modelaapi/services/llm/v1/llm.proto

package v1

import (
	context "context"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
)

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
// Requires gRPC-Go v1.32.0 or later.
const _ = grpc.SupportPackageIsVersion7

// LLMServiceClient is the client API for LLMService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
type LLMServiceClient interface {
	ListLLMs(ctx context.Context, in *ListLLMsRequest, opts ...grpc.CallOption) (*ListLLMsResponse, error)
	CreateLLM(ctx context.Context, in *CreateLLMRequest, opts ...grpc.CallOption) (*CreateLLMResponse, error)
	GetLLM(ctx context.Context, in *GetLLMRequest, opts ...grpc.CallOption) (*GetLLMResponse, error)
	UpdateLLM(ctx context.Context, in *UpdateLLMRequest, opts ...grpc.CallOption) (*UpdateLLMResponse, error)
	DeleteLLM(ctx context.Context, in *DeleteLLMRequest, opts ...grpc.CallOption) (*DeleteLLMResponse, error)
}

type lLMServiceClient struct {
	cc grpc.ClientConnInterface
}

func NewLLMServiceClient(cc grpc.ClientConnInterface) LLMServiceClient {
	return &lLMServiceClient{cc}
}

func (c *lLMServiceClient) ListLLMs(ctx context.Context, in *ListLLMsRequest, opts ...grpc.CallOption) (*ListLLMsResponse, error) {
	out := new(ListLLMsResponse)
	err := c.cc.Invoke(ctx, "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/ListLLMs", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServiceClient) CreateLLM(ctx context.Context, in *CreateLLMRequest, opts ...grpc.CallOption) (*CreateLLMResponse, error) {
	out := new(CreateLLMResponse)
	err := c.cc.Invoke(ctx, "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/CreateLLM", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServiceClient) GetLLM(ctx context.Context, in *GetLLMRequest, opts ...grpc.CallOption) (*GetLLMResponse, error) {
	out := new(GetLLMResponse)
	err := c.cc.Invoke(ctx, "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/GetLLM", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServiceClient) UpdateLLM(ctx context.Context, in *UpdateLLMRequest, opts ...grpc.CallOption) (*UpdateLLMResponse, error) {
	out := new(UpdateLLMResponse)
	err := c.cc.Invoke(ctx, "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/UpdateLLM", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServiceClient) DeleteLLM(ctx context.Context, in *DeleteLLMRequest, opts ...grpc.CallOption) (*DeleteLLMResponse, error) {
	out := new(DeleteLLMResponse)
	err := c.cc.Invoke(ctx, "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/DeleteLLM", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// LLMServiceServer is the server API for LLMService service.
// All implementations must embed UnimplementedLLMServiceServer
// for forward compatibility
type LLMServiceServer interface {
	ListLLMs(context.Context, *ListLLMsRequest) (*ListLLMsResponse, error)
	CreateLLM(context.Context, *CreateLLMRequest) (*CreateLLMResponse, error)
	GetLLM(context.Context, *GetLLMRequest) (*GetLLMResponse, error)
	UpdateLLM(context.Context, *UpdateLLMRequest) (*UpdateLLMResponse, error)
	DeleteLLM(context.Context, *DeleteLLMRequest) (*DeleteLLMResponse, error)
	mustEmbedUnimplementedLLMServiceServer()
}

// UnimplementedLLMServiceServer must be embedded to have forward compatible implementations.
type UnimplementedLLMServiceServer struct {
}

func (UnimplementedLLMServiceServer) ListLLMs(context.Context, *ListLLMsRequest) (*ListLLMsResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ListLLMs not implemented")
}
func (UnimplementedLLMServiceServer) CreateLLM(context.Context, *CreateLLMRequest) (*CreateLLMResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method CreateLLM not implemented")
}
func (UnimplementedLLMServiceServer) GetLLM(context.Context, *GetLLMRequest) (*GetLLMResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method GetLLM not implemented")
}
func (UnimplementedLLMServiceServer) UpdateLLM(context.Context, *UpdateLLMRequest) (*UpdateLLMResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method UpdateLLM not implemented")
}
func (UnimplementedLLMServiceServer) DeleteLLM(context.Context, *DeleteLLMRequest) (*DeleteLLMResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method DeleteLLM not implemented")
}
func (UnimplementedLLMServiceServer) mustEmbedUnimplementedLLMServiceServer() {}

// UnsafeLLMServiceServer may be embedded to opt out of forward compatibility for this service.
// Use of this interface is not recommended, as added methods to LLMServiceServer will
// result in compilation errors.
type UnsafeLLMServiceServer interface {
	mustEmbedUnimplementedLLMServiceServer()
}

func RegisterLLMServiceServer(s grpc.ServiceRegistrar, srv LLMServiceServer) {
	s.RegisterService(&LLMService_ServiceDesc, srv)
}

func _LLMService_ListLLMs_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ListLLMsRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).ListLLMs(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/ListLLMs",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).ListLLMs(ctx, req.(*ListLLMsRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMService_CreateLLM_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(CreateLLMRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).CreateLLM(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/CreateLLM",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).CreateLLM(ctx, req.(*CreateLLMRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMService_GetLLM_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(GetLLMRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).GetLLM(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/GetLLM",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).GetLLM(ctx, req.(*GetLLMRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMService_UpdateLLM_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(UpdateLLMRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).UpdateLLM(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/UpdateLLM",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).UpdateLLM(ctx, req.(*UpdateLLMRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMService_DeleteLLM_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(DeleteLLMRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).DeleteLLM(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/github.com.metaprov.modelaapi.services.llm.v1.LLMService/DeleteLLM",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).DeleteLLM(ctx, req.(*DeleteLLMRequest))
	}
	return interceptor(ctx, in, info, handler)
}

// LLMService_ServiceDesc is the grpc.ServiceDesc for LLMService service.
// It's only intended for direct use with grpc.RegisterService,
// and not to be introspected or modified (even as a copy)
var LLMService_ServiceDesc = grpc.ServiceDesc{
	ServiceName: "github.com.metaprov.modelaapi.services.llm.v1.LLMService",
	HandlerType: (*LLMServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "ListLLMs",
			Handler:    _LLMService_ListLLMs_Handler,
		},
		{
			MethodName: "CreateLLM",
			Handler:    _LLMService_CreateLLM_Handler,
		},
		{
			MethodName: "GetLLM",
			Handler:    _LLMService_GetLLM_Handler,
		},
		{
			MethodName: "UpdateLLM",
			Handler:    _LLMService_UpdateLLM_Handler,
		},
		{
			MethodName: "DeleteLLM",
			Handler:    _LLMService_DeleteLLM_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "github.com/metaprov/modelaapi/services/llm/v1/llm.proto",
}
